{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHHhCYMHORrD",
        "outputId": "d50d1ce8-ec1c-42e7-8272-b122a1bf9870"
      },
      "outputs": [],
      "source": [
        "!git config --global user.name = 'MaheeR15'\n",
        "!git config --global user.email = 'mahee.rathod15@gmail.com'\n",
        "!git config --global user.password = 'Mhermione15'\n",
        "\n",
        "token = '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'\n",
        "username = 'G30rg3ab'\n",
        "repo = 'Computer-Vision-Project'\n",
        "\n",
        "!git clone https://{token}@github.com/{username}/{repo}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9HUVsvIaj3F",
        "outputId": "869de76f-0e2a-4f8f-8bd7-e366fd9807b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "# Importing packages\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import numpy as n\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#sys.path.append(\"/content/Computer-Vision-Project\")\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDdr3VHEQXYv",
        "outputId": "9f30f3f0-4602-4ffd-d741-50168233222e"
      },
      "outputs": [],
      "source": [
        "%cd Computer-Vision-Project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtJf_vXgQo6W",
        "outputId": "bc330720-11be-47f9-9a38-ee6d905a2560"
      },
      "outputs": [],
      "source": [
        "# install necessary packages\n",
        "!pip install torchmetrics\n",
        "!pip install boto3\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CsIJ63PYQGxN"
      },
      "outputs": [],
      "source": [
        "# importing custom packages\n",
        "import segmentation\n",
        "from segmentation.utils import preprocessing, model_utils, traininglog\n",
        "from segmentation import show\n",
        "from segmentation.dataset import CVDataset\n",
        "from segmentation.eval import CVDatasetPredictions\n",
        "from segmentation.constants import VisualisationConstants\n",
        "from segmentation.metrics import Accuracy, Dice, IOU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGyx4QS_Q0sA",
        "outputId": "7a32f6c5-9fee-498b-d96d-e90998b7cb22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# define the device\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")  # Use MPS backend for Apple Silicon\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Use CUDA if available\n",
        "else:\n",
        "    device = torch.device(\"cpu\")   # Default to CPU\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3BsXbtxcQm8V"
      },
      "outputs": [],
      "source": [
        "# training and validation split of data\n",
        "# with augmentation and preprocessing\n",
        "\n",
        "# defining full paths of data\n",
        "DATA_DIR = 'Dataset/'\n",
        "y_trainVal_dir = os.path.join(DATA_DIR, 'TrainVal/label')\n",
        "x_trainVal_dir = os.path.join(DATA_DIR, 'TrainVal/color')\n",
        "X_train_fps, X_val_fps, y_train_fps, y_val_fps = preprocessing.train_val_split(x_trainVal_dir, y_trainVal_dir, 0.2)\n",
        "\n",
        "# setting training, and validation augmentation and prprocessing\n",
        "train_augmentation = preprocessing.get_training_augmentation()\n",
        "train_preprocessing = preprocessing.get_preprocessing()\n",
        "validation_augmentation = preprocessing.get_validation_augmentation()\n",
        "\n",
        "# creating training and validation datasets\n",
        "train_dataset = CVDataset(X_train_fps, y_train_fps, augmentation = train_augmentation, preprocessing = train_preprocessing)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 4, shuffle = True, num_workers = 0)\n",
        "\n",
        "validation_dataset = CVDataset(X_val_fps, y_val_fps, augmentation=validation_augmentation, preprocessing=train_preprocessing)\n",
        "val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zoyxH1ZbJiYP"
      },
      "outputs": [],
      "source": [
        "def calculate_class_distribution(dataset):\n",
        "    \"\"\"\n",
        "    Calculate the class distribution in the dataset's masks.\n",
        "\n",
        "    Args:\n",
        "    dataset: PyTorch dataset object that returns image, mask pairs\n",
        "\n",
        "    Returns:\n",
        "    class_counts: A dictionary with class IDs as keys and their pixel counts as values\n",
        "    total_pixels: Total number of pixels processed across all masks\n",
        "    \"\"\"\n",
        "    class_counts = {0: 0, 1: 0, 2: 0}  # Assuming 3 classes (0, 1, 2)\n",
        "    total_pixels = 0\n",
        "\n",
        "    for img, mask in dataset:\n",
        "        # Flatten the mask to make it easier to count pixels per class\n",
        "        mask = mask.view(-1)  # Flatten the mask to 1D\n",
        "\n",
        "        # Count the occurrences of each class in the mask\n",
        "        unique, counts = torch.unique(mask, return_counts=True)\n",
        "        for u, c in zip(unique, counts):\n",
        "            # Check if the class is in the dictionary, add it if not\n",
        "            if int(u) not in class_counts:\n",
        "              if int(u) != 255:\n",
        "                class_counts[int(u)] = 0\n",
        "\n",
        "            #Only include pixels with label values 0, 1, or 2\n",
        "            if int(u) in class_counts:\n",
        "              class_counts[int(u)] += c.item() # Accumulate the pixel counts per class\n",
        "\n",
        "        total_pixels += mask.numel()  # Add the number of pixels in this mask\n",
        "\n",
        "    return class_counts, total_pixels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cRgysr6Uco7",
        "outputId": "e4912c2d-e7bc-4f8c-f1d2-1aa01d201c47"
      },
      "outputs": [],
      "source": [
        "# Compute class counts using function\n",
        "\n",
        "class_counts, total_pixels = calculate_class_distribution(train_dataset)\n",
        "class_weights = {class_id: total_pixels / (len(class_counts) * count) for class_id, count in class_counts.items()}\n",
        "class_weights_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device)\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Kmy4G78GbHTw"
      },
      "outputs": [],
      "source": [
        "# Define the autoencoder class\n",
        "\n",
        "# Define the Autoencoder class\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # (C, H, W) â†’ (64, H/2, W/2)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # (128, H/4, W/4)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), # (256, H/8, W/8)\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()  # Outputs values in range [0,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nd8K3kbHrMcF"
      },
      "outputs": [],
      "source": [
        "# Create a training function to train the autoencoder\n",
        "\n",
        "def train_autoencoder(num_epochs, X_train_fps, y_train_fps, X_val_fps, y_val_fps, autoencoder, log_filename):\n",
        "  '''\n",
        "  Trains autoencoder on images without labels\n",
        "\n",
        "  Parameters\n",
        "  num_epochs: Number of training epochs\n",
        "  X_train_fps: Full paths of training images\n",
        "  y_train_fps: Full paths of training labels\n",
        "  X_val_fps: Full paths of validation images\n",
        "  y_val_fps: Full paths of validation labels\n",
        "  autoencoder: Autoencoder object\n",
        "  log_filename: File name for the log file \n",
        "  '''\n",
        "\n",
        "  # Set autoencoder to training mode\n",
        "  autoencoder.train()\n",
        "\n",
        "  # Data preprocessing & augmentation\n",
        "  train_augmentation = preprocessing.get_training_augmentation()\n",
        "  train_preprocessing = preprocessing.get_preprocessing()\n",
        "  validation_augmentation = preprocessing.get_validation_augmentation()\n",
        "\n",
        "  # Training dataset\n",
        "  train_dataset = CVDataset(X_train_fps, y_train_fps, augmentation=train_augmentation, preprocessing=train_preprocessing)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "  # Validation dataset (only images, no labels)\n",
        "  validation_dataset = CVDataset(X_val_fps, y_val_fps, augmentation=validation_augmentation, preprocessing=train_preprocessing)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
        "\n",
        "  # Loss function and optimizer\n",
        "  criterion = nn.MSELoss()  # Reconstruction loss\n",
        "  optimizer = optim.Adam(autoencoder.parameters(), lr=1e-4)  # Optimizer\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      autoencoder.train()\n",
        "      running_loss = 0.0\n",
        "\n",
        "      # Training phase\n",
        "      for images,_ in train_loader:\n",
        "          images = images.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = autoencoder(images)  # Forward pass\n",
        "          loss = criterion(outputs, images)  # Compare reconstruction with input\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "      # Validation phase\n",
        "      autoencoder.eval()\n",
        "      val_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for images,_ in val_loader:\n",
        "              images = images.to(device)\n",
        "\n",
        "              outputs = autoencoder(images)  # Forward pass\n",
        "              loss = criterion(outputs, images)  # Compute loss\n",
        "              val_loss += loss.item()\n",
        "\n",
        "      avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "\n",
        "    # Log results to CSV\n",
        "      traininglog.log_training(log_filename, epoch=epoch+1, training_loss=avg_train_loss, validation_loss=avg_val_loss)\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "#train_autoencoder(1, X_train_fps, y_train_fps, autoencoder)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QVuOOT0Sofja"
      },
      "outputs": [],
      "source": [
        "class SegmentationDecoder2(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(SegmentationDecoder2, self).__init__()\n",
        "\n",
        "        # Decoder layers with skip connections\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        self.relu1 = nn.ReLU(True)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU(True)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.relu3 = nn.ReLU(True)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)  # Output logits\n",
        "\n",
        "    def forward(self, x, encoder_features=None):\n",
        "        # Decoder process\n",
        "        x = self.up1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        if encoder_features is not None and len(encoder_features) > 0:\n",
        "            x = torch.cat([x, encoder_features[-1]], dim=1)  # Skip connection\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        if encoder_features is not None and len(encoder_features) > 1:\n",
        "            x = torch.cat([x, encoder_features[-2]], dim=1)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b-uJ3U2YPGSi"
      },
      "outputs": [],
      "source": [
        "# Define the Segmentation Decoder class\n",
        "\n",
        "class SegmentationDecoder(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(SegmentationDecoder, self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(32, num_classes, kernel_size=1)  # Output logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmjTzNRpZmE2"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    111\u001b[39m autoencoder_tst = Autoencoder().to(device)\n\u001b[32m    112\u001b[39m class_weights = [\u001b[32m0.64853\u001b[39m, \u001b[32m2.70486\u001b[39m, \u001b[32m1.38\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mtrain_segmentation_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_fps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_fps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_fps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder_tst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_decoder_tst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msegment.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mtrain_segmentation_decoder\u001b[39m\u001b[34m(num_epochs, X_train_fps, y_train_fps, X_val_fps, y_val_fps, class_weights, autoencoder, segmentation_decoder, log_filename)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# pass images through frozen encoder\u001b[39;00m\n\u001b[32m     60\u001b[39m encoded = pretrained_encoder(images)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m outputs = \u001b[43msegmentation_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     63\u001b[39m labels = labels.long()\n\u001b[32m     65\u001b[39m loss = criterion(outputs, labels)  \u001b[38;5;66;03m# Compare reconstruction with input\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mSegmentationDecoder2.forward\u001b[39m\u001b[34m(self, x, encoder_features)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_features) > \u001b[32m1\u001b[39m:\n\u001b[32m     34\u001b[39m     x = torch.cat([x, encoder_features[-\u001b[32m2\u001b[39m]], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m x = \u001b[38;5;28mself\u001b[39m.bn3(x)\n\u001b[32m     38\u001b[39m x = \u001b[38;5;28mself\u001b[39m.relu3(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cv/lib/python3.12/site-packages/torch/nn/modules/conv.py:952\u001b[39m, in \u001b[36mConvTranspose2d.forward\u001b[39m\u001b[34m(self, input, output_size)\u001b[39m\n\u001b[32m    947\u001b[39m num_spatial_dims = \u001b[32m2\u001b[39m\n\u001b[32m    948\u001b[39m output_padding = \u001b[38;5;28mself\u001b[39m._output_padding(\n\u001b[32m    949\u001b[39m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m.stride, \u001b[38;5;28mself\u001b[39m.padding, \u001b[38;5;28mself\u001b[39m.kernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    950\u001b[39m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m.dilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Now we will train the decoder model\n",
        "\n",
        "\n",
        "def train_segmentation_decoder(num_epochs, X_train_fps, y_train_fps, X_val_fps, y_val_fps, class_weights, autoencoder, segmentation_decoder, log_filename):\n",
        "  '''\n",
        "  num_epochs: Number of training epochs\n",
        "  X_train_fps: Full paths of training images\n",
        "  y_train_fps: Full paths of training labels\n",
        "  X_val_fps: Full paths of validation images\n",
        "  y_val_fps: Full paths of validation labels\n",
        "  class_weights: weights to use when calculating cross entropy loss (array: [weight for class 0, weight for class 1, weight for class 2])\n",
        "  autoencoder: trained autoencoder object\n",
        "  segmentation_decoder: segmentation decoder object\n",
        "  log_filename: Name of file for logging\n",
        "  '''\n",
        "\n",
        "  # seting segmentation decoder to training mode\n",
        "  segmentation_decoder.train()\n",
        "  autoencoder.eval()\n",
        "\n",
        "  # Getting the augmented training images from full paths\n",
        "  train_augmentation = preprocessing.get_training_augmentation()\n",
        "  train_preprocessing = preprocessing.get_preprocessing()\n",
        "  validation_augmentation = preprocessing.get_validation_augmentation()\n",
        "\n",
        "  # get training dataset\n",
        "  train_dataset = CVDataset(X_train_fps, y_train_fps, augmentation = train_augmentation, preprocessing = train_preprocessing)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 4, shuffle = True, num_workers = 0)\n",
        "\n",
        "  # Get validation dataset\n",
        "  validation_dataset = CVDataset(X_val_fps, y_val_fps, augmentation = validation_augmentation, preprocessing = train_preprocessing)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 4, shuffle = False, num_workers = 0)\n",
        "\n",
        "  # obtain encoder from autoencoder and freeze it\n",
        "  pretrained_encoder = autoencoder.encoder.to(device)\n",
        "  pretrained_encoder.eval()\n",
        "\n",
        "  for p in pretrained_encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "  # Now setting loss and optimizer\n",
        "  class_weights = torch.tensor(class_weights).to(device)\n",
        "  criterion = nn.CrossEntropyLoss(weight = class_weights, ignore_index = 255) # Reconstruction loss\n",
        "  optimizer = torch.optim.Adam(segmentation_decoder.parameters(), lr=5e-5)\n",
        "  #optimizer = torch.optim.SGD(segmentation_decoder.parameters(), lr=5e-4, momentum=0.9)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "\n",
        "  # Now training decoder\n",
        "  for epoch in range(num_epochs):\n",
        "    segmentation_decoder.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # pass images through frozen encoder\n",
        "        encoded = pretrained_encoder(images)\n",
        "        outputs = segmentation_decoder(encoded)  # Forward pass\n",
        "\n",
        "        labels = labels.long()\n",
        "\n",
        "        loss = criterion(outputs, labels)  # Compare reconstruction with input\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    segmentation_decoder.eval()  # Set to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    iou_metric = IOU(classes = [0,1,2])\n",
        "    dice_metric = Dice(classes = [0,1,2])\n",
        "    accuracy_metric = Accuracy(ignore_class = 255)\n",
        "    \n",
        "    with torch.no_grad():  # No gradient computation\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device).long()\n",
        "\n",
        "            encoded = pretrained_encoder(images)\n",
        "            outputs = segmentation_decoder(encoded)  # Forward pass\n",
        "            pred_masks = torch.argmax(outputs, dim =1)\n",
        "\n",
        "            iou_metric.update(pred_masks, labels)\n",
        "            dice_metric.update(pred_masks, labels)\n",
        "            accuracy_metric.update(pred_masks, labels)\n",
        "\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            val_loss += loss.item()\n",
        "            \n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    mean_iou = iou_metric.compute()\n",
        "    accuracy = accuracy_metric.compute()\n",
        "    dice = dice_metric.compute()\n",
        "\n",
        "    # Reduce learning rate if loss plateaus\n",
        "    scheduler.step(avg_train_loss)\n",
        "\n",
        "    traininglog.log_training(log_filename, epoch=epoch+1, training_loss=avg_train_loss, validation_loss=avg_val_loss, mean_iou = mean_iou,\n",
        "                             accuracy = accuracy, dice = dice)\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "segmentation_decoder_tst = SegmentationDecoder2().to(device)\n",
        "autoencoder_tst = Autoencoder().to(device)\n",
        "class_weights = [0.64853, 2.70486, 1.38]\n",
        "\n",
        "\n",
        "#train_segmentation_decoder(1, X_train_fps, y_train_fps, X_val_fps, y_val_fps, class_weights, autoencoder_tst, segmentation_decoder_tst, 'segment.csv')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "948mASagsH7Q"
      },
      "outputs": [],
      "source": [
        "# Create new function to train the autoencoder and segmentation decoder in one function\n",
        "\n",
        "\n",
        "def train_autoencoder_decoder(autoencoder_num_epochs, segmentation_num_epochs, X_train_fps, y_train_fps, X_val_fps, y_val_fps, class_weights, autoencoder, segmentation_decoder,\n",
        "                              autoencoder_filename, segmentation_filename):\n",
        "  '''\n",
        "  Trains the autoencoder and segmentation decoder in one function\n",
        "\n",
        "  Parameters\n",
        "  autoencoder_num_epochs: Number of training epochs for the autoencoder\n",
        "  segmentation_num_epochs: Number of training epochs for the segmentation decoder\n",
        "  X_train_fps: Full paths of training images\n",
        "  y_train_fps: Full paths of training labels\n",
        "  X_val_fps: Full paths of validation images\n",
        "  y_val_fps: Full paths of validation labels\n",
        "  class_weights: class weights to use in segmentation decoder training \n",
        "  autoencoder: autoencoder object\n",
        "  segmentation_decoder: segmentation decoder object\n",
        "  autoencoder_filename: file name to save autoencoder training metrics to \n",
        "  segmentation_filename: file name to save segmentation decoder metrics to \n",
        "  '''\n",
        "\n",
        "  # put autoencoder and segmentation decoder into training mode\n",
        "  autoencoder.train()\n",
        "  segmentation_decoder.train()\n",
        "\n",
        "  # train the autoencoder\n",
        "  train_autoencoder(autoencoder_num_epochs, X_train_fps, y_train_fps, X_val_fps, y_val_fps, autoencoder, autoencoder_filename)\n",
        "\n",
        "\n",
        "  # train the segmentation decoder\n",
        "  train_segmentation_decoder(segmentation_num_epochs, X_train_fps, y_train_fps, X_val_fps, y_val_fps, class_weights, autoencoder, segmentation_decoder, segmentation_filename)\n",
        "\n",
        "\n",
        "# Training autoencoder_decoder\n",
        "#train_autoencoder_decoder(3, X_train_fps, y_train_fps, X_val_fps, y_val_fps, autoencoder, segmentation_decoder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# clear cache \n",
        "torch.mps.empty_cache()\n",
        "torch.mps.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjYj7EwgURoJ",
        "outputId": "07ad6dd9-9da6-4e99-d3d5-e780e9ffc3d4"
      },
      "outputs": [],
      "source": [
        "# Training the autoencoder and segmentation decoder\n",
        "\n",
        "# creating instances of autoencoder and segmentation decoder\n",
        "autoencoder = Autoencoder().to(device)\n",
        "segmentation_decoder = SegmentationDecoder2().to(device)\n",
        "\n",
        "# setting the class weights \n",
        "class_weights = [0.6532065125183609, 2.6677354481961246, 1.370546912591959]\n",
        "\n",
        "# training\n",
        "train_autoencoder_decoder(30,100, X_train_fps, y_train_fps, X_val_fps, y_val_fps, class_weights, autoencoder, segmentation_decoder, 'autoencoder_logs.csv', 'segmentation_logs.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjgN0-BdkEV",
        "outputId": "ef6c4a5d-3d2e-4a43-c594-02fa670ef3bf"
      },
      "outputs": [],
      "source": [
        "# saving the trained models\n",
        "state_dict_autoencoder = autoencoder.state_dict()\n",
        "checkpoint_autoencoder = {'state_dict': state_dict_autoencoder,\n",
        "              'epochs': 30,\n",
        "              'learning_rate': 1e-4}\n",
        "model_utils.save_checkpoint(\n",
        "    checkpoint_autoencoder,\n",
        "    os.path.expanduser(\"~/Documents/Computer-Vision-Project/autoencoder.pth\") \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cLjaI_7yYMR",
        "outputId": "71c952b2-f93f-487d-97db-f2cf96d799af"
      },
      "outputs": [],
      "source": [
        "# Getting the trained models\n",
        "\n",
        "# creating an instance of the model\n",
        "autoencoder_loaded = Autoencoder().to(device)\n",
        "segmentation_decoder_loaded = SegmentationDecoder().to(device)\n",
        "\n",
        "# loading the trained models\n",
        "model_utils.load_checkpoint(\"/content/Computer-Vision-Project/autoencoder.pth\", autoencoder_loaded)\n",
        "model_utils.load_checkpoint(\"/content/Computer-Vision-Project/segmentation_decoder.pth\", segmentation_decoder_loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV4qRJ9rxvpX"
      },
      "outputs": [],
      "source": [
        "# Creating a function which makes mask from input image\n",
        "\n",
        "def predict(image, autoencoder, segmentation_decoder, device):\n",
        "  '''\n",
        "  Predicts mask from input image\n",
        "\n",
        "  Parameters\n",
        "  image: input image of size (3, H, W)\n",
        "  autoencoder: trained autoencoder\n",
        "  segmentation_decoder: trained segmentation decoder\n",
        "\n",
        "  Returns\n",
        "  predicted_mask: predicted mask of size (1, H, W)\n",
        "  '''\n",
        "  # move the image to the device and set autoencoder and segmentation_decoder to eval\n",
        "  image = image.to(device)\n",
        "  autoencoder.eval()\n",
        "  segmentation_decoder.eval()\n",
        "\n",
        "  # Encode image using encoder from trained autoencoder\n",
        "  encoded_image = autoencoder.encoder(image.unsqueeze(0))\n",
        "\n",
        "  # segment image using segmentation decoder\n",
        "  decoded_image = segmentation_decoder(encoded_image)\n",
        "  predicted_mask = torch.argmax(decoded_image, dim =1)\n",
        "\n",
        "  return predicted_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "HRkmypKMv9FM",
        "outputId": "6bf3f16a-3b8f-452e-9b8b-cd8e9a88ffdc"
      },
      "outputs": [],
      "source": [
        "validation_dataset = CVDataset(X_val_fps, y_val_fps, augmentation = validation_augmentation, preprocessing = train_preprocessing)\n",
        "\n",
        "val_image, val_label = validation_dataset[0]\n",
        "numpy_val_image = val_image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "train_image, train_label = validation_dataset[2]\n",
        "\n",
        "prediction = predict(train_image, autoencoder, segmentation_decoder, device)\n",
        "prediction = prediction.squeeze(0)\n",
        "prediction_mask = show.colorise_mask(prediction, VisualisationConstants.palette)\n",
        "\n",
        "show.visualise_data(image = train_image, mask = prediction_mask)\n",
        "\n",
        "print(torch.unique(prediction))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdIJgxpAZdYf",
        "outputId": "1f76d336-d0e0-44f7-fa81-896e31420bc2"
      },
      "outputs": [],
      "source": [
        "# Testing on an image from the data\n",
        "\n",
        "validation_dataset = CVDataset(X_val_fps, y_val_fps, augmentation = validation_augmentation, preprocessing = train_preprocessing)\n",
        "\n",
        "val_image, val_label = validation_dataset[0]\n",
        "\n",
        "#decoded_image = predict(val_image, autoencoder, segmentation_decoder, device)\n",
        "\n",
        "\n",
        "\n",
        "# finding IOU\n",
        "predictions_eval = CVDatasetPredictions(validation_dataset, device=device)\n",
        "\n",
        "predictions_eval.set_prediction_fn(predict, autoencoder = autoencoder, segmentation_decoder = segmentation_decoder, device = device)\n",
        "predictions_eval.mean_IoU(progress_bar = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaVoGXDDvUFp"
      },
      "outputs": [],
      "source": [
        "# Part C - CLIP segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7E-S-ecShU6",
        "outputId": "5d3bbde2-75cf-4b45-95cb-a58c9cbdafec"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIziAW61uy3n",
        "outputId": "c1847385-4d09-49ac-be4c-acf7f55c646c"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained CLIP model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "visual_encoder = model.visual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esvHRDHwkN-Y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models.segmentation as segmentation\n",
        "\n",
        "# Create CLIP segmentation decoder class\n",
        "class CLIPSegmentationDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(CLIPSegmentationDecoder, self).__init__()\n",
        "\n",
        "        # Adjust the upsampling layers to produce 416x256 output\n",
        "        self.upconv1 = nn.ConvTranspose2d(in_channels, 512, kernel_size=4, stride=2, padding=1)  # From [7x7] to [14x14]\n",
        "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)  # From [14x14] to [28x28]\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # From [28x28] to [56x56]\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # From [56x56] to [112x112]\n",
        "        self.upconv5 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # From [112x112] to [224x224]\n",
        "\n",
        "        # Final convolution to get the exact size of 416x256\n",
        "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)  # Output segmentation map\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Upsampling through the decoder layers\n",
        "        x = self.upconv1(x)  # From [7x7] to [14x14]\n",
        "        x = self.upconv2(x)  # From [14x14] to [28x28]\n",
        "        x = self.upconv3(x)  # From [28x28] to [56x56]\n",
        "        x = self.upconv4(x)  # From [56x56] to [112x112]\n",
        "        x = self.upconv5(x)  # From [112x112] to [224x224]\n",
        "\n",
        "        # Now we need to resize the output from [224x224] to [416x256]\n",
        "        x = nn.functional.interpolate(x, size=(256, 416), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Final output layer to get the desired segmentation output\n",
        "        x = self.final_conv(x)  # Output shape: [1, num_classes, 416, 256]\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ytBThvOjxz",
        "outputId": "13a1fd17-cccc-446a-bde3-1470e7959d0a"
      },
      "outputs": [],
      "source": [
        "# Creating function that encodes image using CLIP encoder\n",
        "\n",
        "def CLIP_encoder(input_images):\n",
        "  '''\n",
        "  Encodes images using CLIP encoder\n",
        "\n",
        "  Parameters:\n",
        "  input_images: Images to be encoded with shape [batch_size, 3, 256, 416]\n",
        "\n",
        "  Returns:\n",
        "  Encoded image of shape [batch_size, 49, 768]\n",
        "  '''\n",
        "\n",
        "  # list to store encoded images\n",
        "  encoded_images = []\n",
        "\n",
        "  for img in input_images:\n",
        "    # resizing the images to be [224 x 224] for CLIP encoder\n",
        "    input_image_pil = T.ToPILImage()(img)\n",
        "    resized_image_pil = preprocess(input_image_pil)  # Resize using CLIP's preprocess function\n",
        "    resized_image = resized_image_pil.unsqueeze(0).to(device)\n",
        "\n",
        "    # Now encode image using CLIP encoder\n",
        "    with torch.no_grad():\n",
        "      x = visual_encoder.conv1(resized_image)  # Shape: [1, 768, H/32, W/32] for ViT-B/32\n",
        "      x = x.reshape(1, 768, -1).permute(0, 2, 1)  # Shape: [1, num_patches = 49, 768]\n",
        "      pos_embed = visual_encoder.positional_embedding[1:, :].unsqueeze(0)\n",
        "      x = x + pos_embed  # Add positional embeddings\n",
        "      x = visual_encoder.ln_pre(x) # layer normalisation\n",
        "\n",
        "      # Pass through Transformer Encoder\n",
        "      x = x.permute(1, 0, 2)  # Shape: [num_patches, 1, 768]\n",
        "      x = visual_encoder.transformer(x)  # Shape: [num_patches, 1, 768]\n",
        "      x = x.permute(1, 0, 2)  # Shape: [1, num_patches, 768]\n",
        "      encoded_images.append(x)  # Store the encoded image\n",
        "\n",
        "  # Stack all encoded images into a single batch tensor\n",
        "  encoded_images = torch.cat(encoded_images, dim=0)\n",
        "  return(encoded_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwxoftpFsdvd"
      },
      "outputs": [],
      "source": [
        "# testing the CLIP encoder\n",
        "\n",
        "images,labels = train_loader.__iter__().__next__()\n",
        "\n",
        "images = images.to(device)\n",
        "\n",
        "encoded = CLIP_encoder(images)\n",
        "\n",
        "batch_size, num_patches, embed_dim = encoded.shape  # [batch_size, 49, 768]\n",
        "H, W = int(num_patches ** 0.5), int(num_patches ** 0.5)  # Assuming square patches (7x7)\n",
        "\n",
        "# Reshape into CNN-friendly format [batch_size, 768, H, W]\n",
        "encoded = encoded.permute(0, 2, 1).reshape(batch_size, embed_dim, H, W)  # [batch_size, 768, 7, 7]\n",
        "\n",
        "encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKKZ3Fu8NtzZ"
      },
      "outputs": [],
      "source": [
        "# Function to train the CLIP segmentation Decoder\n",
        "def train_segmentation_decoder(num_epochs, X_train_fps, y_train_fps, X_val_fps, y_val_fps, CLIP_encoder, CLIP_segmentation_decoder, log_filename):\n",
        "  '''\n",
        "  Trains the Segmentation Decoder for images encoded by CLIP encoder\n",
        "\n",
        "  Parameters:\n",
        "  num_epochs: Number of training epochs\n",
        "  X_train_fps: Full paths of training images\n",
        "  y_train_fps: Full paths of training labels\n",
        "  X_val_fps: Full paths of validation images\n",
        "  y_val_fps: Full paths of validation labels\n",
        "  CLIP_encoder : function whcih takes in image and outputs CLIP encoded image\n",
        "  CLIP_segmentation_decoder: CLIP segmentation decoder object\n",
        "  log_filename: name of file to store log of training data in \n",
        "  '''\n",
        "\n",
        "  # seting segmentation decoder to training mode\n",
        "  CLIP_segmentation_decoder.train()\n",
        "\n",
        "  # Getting the augmented training images from full paths\n",
        "  train_augmentation = preprocessing.get_training_augmentation()\n",
        "  train_preprocessing = preprocessing.get_preprocessing()\n",
        "  validation_augmentation = preprocessing.get_validation_augmentation()\n",
        "\n",
        "  train_dataset = CVDataset(X_train_fps, y_train_fps, augmentation = train_augmentation, preprocessing = train_preprocessing)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True, num_workers = 0)\n",
        "\n",
        "  # Get validation dataset\n",
        "  validation_dataset = CVDataset(X_val_fps, y_val_fps, augmentation = validation_augmentation, preprocessing = train_preprocessing)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "  # Now setting loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index = 255) # Reconstruction loss\n",
        "  optimizer = torch.optim.Adam(CLIP_segmentation_decoder.parameters(), lr=5e-3)\n",
        "\n",
        "\n",
        "  # Now training decoder\n",
        "  for epoch in range(num_epochs):\n",
        "    val_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # pass images through CLIP encoder\n",
        "        encoded = CLIP_encoder(images)\n",
        "\n",
        "        # Reshape the feature vector into a 2D feature map (for upsampling in the decoder)\n",
        "        batch_size, num_patches, embed_dim = encoded.shape  # [batch_size, 49, 768]\n",
        "        H, W = int(num_patches ** 0.5), int(num_patches ** 0.5)  # Assuming square patches (7x7)\n",
        "\n",
        "        # Reshape into CNN-friendly format [batch_size, 768, H, W]\n",
        "        encoded = encoded.permute(0, 2, 1).reshape(batch_size, embed_dim, H, W)  # [batch_size, 768, 7, 7]\n",
        "\n",
        "        outputs = CLIP_segmentation_decoder(encoded)  # Forward pass\n",
        "\n",
        "        labels = labels.long()\n",
        "\n",
        "        loss = criterion(outputs, labels)  # Compare reconstruction with input\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    CLIP_segmentation_decoder.eval()  # Set to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    mean_iou = 0.0\n",
        "    with torch.no_grad():  # No gradient computation\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device).long()\n",
        "\n",
        "            encoded = encoded.permute(0, 2, 1).reshape(batch_size, embed_dim, H, W)  # [batch_size, 768, 7, 7]\n",
        "            outputs = CLIP_segmentation_decoder(encoded)  # Forward pass\n",
        "\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    predictions_eval = CVDatasetPredictions(validation_dataset, device=device)\n",
        "\n",
        "    predictions_eval.set_prediction_fn(predict, autoencoder = autoencoder, segmentation_decoder = segmentation_decoder, device = device)\n",
        "    mean_iou = predictions_eval.mean_IoU(progress_bar = False)\n",
        "            \n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    traininglog.log_training(log_filename, epoch=epoch+1, training_loss=avg_train_loss, validation_loss=avg_val_loss, IoU = mean_iou)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH4SkdHyTBWO",
        "outputId": "9750e2ae-0e92-4547-8dfb-97c204242b67"
      },
      "outputs": [],
      "source": [
        "# initialise CLIP segmentation decoder\n",
        "CLIP_segmentation_decoder = CLIPSegmentationDecoder(in_channels=768, num_classes=3).to(device)\n",
        "\n",
        "# train CLIP segmentation decoder\n",
        "train_segmentation_decoder(1, X_train_fps, y_train_fps, X_val_fps, y_val_fps, CLIP_encoder, CLIP_segmentation_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7rjgmJqjfeq"
      },
      "outputs": [],
      "source": [
        "# Function to CLIP encode and segment image\n",
        "\n",
        "def predict_CLIP(image, CLIP_encoder, CLIP_segmentation_decoder, device):\n",
        "  '''\n",
        "  Predicts mask from input image\n",
        "\n",
        "  Parameters\n",
        "  image: input image of size (3, H, W)\n",
        "  CLIP_encoder : function whcih takes in image and outputs CLIP encoded image\n",
        "  CLIP_segmentation_decoder: CLIP segmentation decoder object\n",
        "  device: device to run the model on\n",
        "\n",
        "  Returns\n",
        "  predicted_mask: predicted mask of size (1, H, W)\n",
        "  '''\n",
        "\n",
        "  # Encode image\n",
        "  encoded = CLIP_encoder(image.unsqueeze(0))\n",
        "\n",
        "  # reshape image for decoder\n",
        "  encoded = encoded.reshape(1, 49, 768)\n",
        "\n",
        "  # Now reshape to [1, 768, H/32, W/32] so we can process it with the decoder\n",
        "  encoded = encoded.permute(0, 2, 1).reshape(1, 768, 7, 7)  # Shape: [1, 768, 7, 7]\n",
        "\n",
        "  # decode image\n",
        "  decoded = CLIP_segmentation_decoder(encoded)\n",
        "  predicted_mask = torch.argmax(decoded, dim =1)\n",
        "\n",
        "  return predicted_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "AmA5obGynOnU",
        "outputId": "33140fe7-6a7d-4d58-85ef-915c8d281b50"
      },
      "outputs": [],
      "source": [
        "# Testing predictor\n",
        "\n",
        "image, label = train_dataset[10]\n",
        "\n",
        "predicted = predict_CLIP(image, CLIP_encoder, CLIP_segmentation_decoder, device)\n",
        "predicted = predicted.squeeze(0)\n",
        "prediction_mask = show.colorise_mask(predicted, VisualisationConstants.palette)\n",
        "\n",
        "show.visualise_data(image = image, mask = prediction_mask)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
